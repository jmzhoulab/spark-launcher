
# evn
hadoop.conf.dir = /Users/stargraph/workspace/fork/gdp-ksyun/gdp-ksyun-job/src/main/resource
yarn.conf.dir = /Users/stargraph/workspace/fork/gdp-ksyun/gdp-ksyun-job/src/main/resource
spark.home = /Users/stargraph/opt/spark2

app.resource = /Users/stargraph/workspace/self/spark-launcher/lib/graph4s-1.0.jar
main.class = mu.atlas.boot.Algorithm

master = yarn
deploy.mode = cluster
jars = /Users/stargraph/workspace/self/spark-launcher/lib/graph4s-1.0.jar

# 如果是yarn的cluster模式需要通过此参数指定算法所有依赖包在hdfs上的路径
spark.yarn.jars = hdfs://hadoop01.sz.haizhi.com:8020/train_spark_launcher/jars/*.jar
app.name =  Kcore
app.args = --alg.name Kcore --input /user/work/graph-algorithm/zhoujm/temp/scale_relate/part-00000 --output /user/work/graph-algorithm/zhoujm/temp/kcore

log.level = debug

spark.hadoop.user.name = work

spark.queue = work
